{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 1 (Practice)\n",
    "To solve this task, you will write a lot of code to try several machine learning methods for classification and regression.\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember that seminars, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, whole life?).\n",
    "\n",
    "* If you want an easy life, you have to use **BUILT-IN METHODS** of `sklearn` library instead of writing tons of your own code. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "\n",
    "* To do this part of homework, you have to write **CODE** directly inside specified places inside notebook **CELLS**.\n",
    "\n",
    "* In some problems you are asked to provide short discussion of the results. In these cases you have to create **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
    "\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**. So make sure that you did everything required in the task\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if the reviewer decides to execute all, after all the computation he will obtain exactly the same solution (with all the corresponding plots) as in your uploaded notebook. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudorandomness.\n",
    "\n",
    "* Your code must be clear to the reviewer. For this purpose, try to include neccessary comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY** without any additional comments.\n",
    "\n",
    "* Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameters and set it to `-1` to use all the cores.\n",
    "\n",
    "To begin with, let's import the essential (for this assignment) libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1. Boosting, part 1. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Boosting Machines (BM) are a family of widely popular and effective methods for classification and regression tasks. The main idea behind BMs is that **combining weak learners**, that perform slightly better than random, can result in **strong learning models**.\n",
    "\n",
    "> AdaBoost utilizes the greedy training approach: firstly we train the weak learners (they are later called `base_classifiers`) on the whole dataset and in the next iterations we train the model on the samples, on the which the previous models have performed poorly. This behavior is acheived by reweighting the training samples during each algorithm's step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The task:\n",
    "\n",
    "In this exercise you will be asked to implement one of the earlier variants of BMs - **AdaBoost** and compare it to the already existing `sklearn` implementation. The key steps are:\n",
    "\n",
    "* Complete the `ada_boost_alpha` and `ada_boost_distribution` functions\n",
    "\n",
    "* Complete the `.fit` method of `Boosting` class\n",
    "\n",
    "* Complete the `.predict` method of `Boosting` class\n",
    "\n",
    "The pseudocode for AdaBoost can be found in [Lecture 7](https://github.com/adasegroup/ML2020_lectures).\n",
    "\n",
    "##### criteria\n",
    "\n",
    "the decision boundary of the final implementation should look reasonably identical to the model from `sklearn`, and should achieve accuracy close to `scikit` :\n",
    "\n",
    "$$\n",
    "    |\\text{your_accuracy} - \\text{sklearn_accuracy}| \\leq 0.005\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place for ypur solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the dataset\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=0)\n",
    "\n",
    "# for convenience convert labels from {0, 1} to {-1, 1}\n",
    "y[y == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 30),\n",
    "                     np.linspace(y_min, y_max, 30))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='.', c=y_test, cmap=cm_bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ada_boost_alpha` - function, which calculates the weights of the linear combination of the classifiers\n",
    "* `ada_boost_distribution` - function, which calculates sample weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement htese procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let us define functions to calculate alphas and distributions for AdaBosot algorithm\n",
    "\n",
    "def ada_boost_alpha(y, y_pred_t, distribution):\n",
    "    \"\"\"\n",
    "    y_pred_t is a prediction of the t-th base classifier\n",
    "    \"\"\"\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost_distribution(y, y_pred_t, distribution, alpha_t):\n",
    "    \"\"\"\n",
    "    y_pred_t is a prediction of the t-th base classifier\n",
    "    \"\"\"\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom boosting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boosting():\n",
    "    \"\"\"\n",
    "    Generic class for construction of boosting models\n",
    "    \n",
    "    :param n_estimators: int, number of estimators (number of boosting rounds)\n",
    "    :param base_classifier: callable, a function that creates a weak estimator. Weak estimator should support sample_weight argument\n",
    "    :param get_alpha: callable, a function, that calculates new alpha given current distribution, prediction of the t-th base estimator,\n",
    "                      boosting prediction at step (t-1) and actual labels\n",
    "    :param get_distribution: callable, a function, that calculates samples weights given current distribution, prediction, alphas and actual labels\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=50, base_classifier=None,\n",
    "                 get_alpha=ada_boost_alpha, update_distribution=ada_boost_distribution):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_classifier = base_classifier\n",
    "        self.get_alpha = get_alpha\n",
    "        self.update_distribution = update_distribution\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        distribution = np.ones(n_samples, dtype=float) / n_samples\n",
    "        self.classifiers = []\n",
    "        self.alphas = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # create a new classifier\n",
    "            self.classifiers.append(self.base_classifier())     \n",
    "            self.classifiers[-1].fit(X, y, sample_weight=distribution)\n",
    "\n",
    "            # ======= Your code here ======\n",
    "            \n",
    "            # make a prediction\n",
    "\n",
    "            \n",
    "            #update alphas, append new alpha to self.alphas\n",
    "\n",
    "            \n",
    "            # update distribution and normalize\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        final_predictions = np.zeros(X.shape[0])\n",
    "    \n",
    "        # ====== Your code here ======\n",
    "        #get the weighted votes of the classifiers\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "max_depth = 5\n",
    "n_estimators = 100\n",
    "\n",
    "get_base_clf = lambda: DecisionTreeClassifier(max_depth=max_depth)\n",
    "ada_boost1 = Boosting(n_estimators=n_estimators,\n",
    "                     base_classifier=get_base_clf)\n",
    "ada_boost1.fit(X_train, y_train)\n",
    "\n",
    "ada_boost_sklearn = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth),\n",
    "                                       algorithm=\"SAMME\",\n",
    "                                       n_estimators=n_estimators)\n",
    "\n",
    "ada_boost_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [ada_boost1, ada_boost_sklearn]\n",
    "names = ['ada_boost', 'ada_boost_sklearn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test ensemble classifier\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i, clf in enumerate(classifiers):\n",
    "    prediction = clf.predict(X_test)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    ax = plt.subplot(1, len(classifiers), i + 1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, alpha=0.5)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(names[i])\n",
    "\n",
    "    print('accuracy {}: {}'.format(names[i], (prediction == y_test).sum() * 1. / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2. Boosting, part 2. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For adaboost we have computed each alpha according to the formula:\n",
    "\n",
    "$$\\alpha_t = \\frac{1}{2}\\log{\\frac{1-N_T(h_T, w_T )}{N_T(h_T,w_t)}}$$\n",
    "\n",
    "In the next task you will be asked to instead recompute all of the alphas after adding another estimator to the composition of models, i.e. when the new classifier is fitted to the weighted data samples, the new alphas should be recomputed by directly minimizing the exponential loss, for all the avaliable estimators. Take into account, that at each step of the boosting algorithm **all** alphas and, consequntly, **all** sample weights should be recomputed from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**:\n",
    "* Complete the `recompute_alpha` and `recompute_distribution` functions\n",
    "* Complete the `.fit` method of `RecomputeBoosting` class\n",
    "* Complete the `.predict` method of `RecomputeBoosting` class\n",
    "* Plot the final alphas (last calculated in case of recomputed alphas)\n",
    "* plot the history scores of the resulting model at each iteration (use ```predict```, ```score``` and ```construct_alpha_history``` functions defined below) for both RecomputedBoosting and AdaBoost (it can be either your own implementation from Exercise 1.1 or the one from sklearn - in this case use ```.estimator_weights_```, ```.estimators_```)\n",
    "* Make a conclusion about which method is better and in which case - in order to do that you can additionally vary the parameters of training: number and depth of the estimators, noise of the data (moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINTS:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize the exponential loss with respect to alpha, use the [```scipy.optimize.minimize```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the function to minimize (in case of AdaBoost it should be the exponential loss), have a look at the optimization problem formulation of Ridge Regression and at the reguralization parameter([Lecture 2, slide 14](https://github.com/adasegroup/ML2020_lectures/blob/master/lecture2/Lecture_2_Regression.pdf)). Add it in order to guarantee the existence of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_alpha(y, y_pred, C):\n",
    "    \"\"\"\n",
    "    y_pred - is a list of predictions of the existing estimators\n",
    "    C - is a reguralization term    \n",
    "    \"\"\"\n",
    "    # ====== Your code here =====\n",
    "\n",
    "    return alphas\n",
    "    \n",
    "\n",
    "def recompute_distribution(y, y_pred, alphas):\n",
    "    \"\"\"\n",
    "    y_pred - is a list of predictions of the existing estimators\n",
    "    alphas - are the last recomputed alphas\n",
    "    \"\"\"\n",
    "\n",
    "    # ====== Your code here =====\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecomputedBoosting():\n",
    "    \"\"\"\n",
    "    Generic class for construction of boosting models\n",
    "    \n",
    "    :param n_estimators: int, number of estimators (number of boosting rounds)\n",
    "    :param base_classifier: callable, a function that creates a weak estimator. Weak estimator should support sample_weight argument\n",
    "    :param get_alpha: callable, a function, that calculates new alpha given current distribution, prediction of the t-th base estimator,\n",
    "                      boosting prediction at step (t-1) and actual labels\n",
    "    :param get_distribution: callable, a function, that calculates samples weights given current distribution, prediction, alphas and actual labels\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=50, base_classifier=None,\n",
    "                 get_alpha=recompute_alpha, update_distribution=recompute_distribution, C=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_classifier = base_classifier\n",
    "        self.get_alpha = get_alpha\n",
    "        self.update_distribution = update_distribution\n",
    "        self.C = C\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        distribution = np.ones(n_samples, dtype=float) / n_samples\n",
    "        self.classifiers = []\n",
    "        \n",
    "        #notice how alpha is a matrix - we will store the history of the updates here\n",
    "        self.alphas = np.zeros((n_estimators, n_estimators))\n",
    "        for i in range(self.n_estimators):\n",
    "            # create a new classifier\n",
    "            self.classifiers.append(self.base_classifier())        \n",
    "            self.classifiers[-1].fit(X, y, sample_weight=distribution)\n",
    "            \n",
    "            \n",
    "            # ======= Your code here ======\n",
    "            # create a list of predictions across all classifiers\n",
    "\n",
    "            # recalculate alphas, add them to the matrix of self.alphas \n",
    "            # NOTE: here, self.alphas is a history of computed alphas at each step i\n",
    "            \n",
    "            \n",
    "            # update distribution and normalize\n",
    "    \n",
    "    def predict(self, X):\n",
    "        final_predictions = np.zeros(X.shape[0])\n",
    "    \n",
    "        # ====== Your code here ======\n",
    "        #get the weighted votes of the classifiers\n",
    "        #do not forget that self.alphas is the whole history of recalculated alphas!\n",
    "\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_boost = RecomputedBoosting(n_estimators=n_estimators,\n",
    "                     base_classifier=get_base_clf, get_alpha=recompute_alpha, \n",
    "                     update_distribution=recompute_distribution, C=1.0)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "ada_boost_sklearn = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth),\n",
    "                                       algorithm=\"SAMME\",\n",
    "                                       n_estimators=n_estimators)\n",
    "\n",
    "ada_boost_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [ada_boost, ada_boost_sklearn]\n",
    "names = ['ada_boost', 'ada_boost_sklearn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test ensemble classifier\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i, clf in enumerate(classifiers):\n",
    "    prediction = clf.predict(X_test)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    ax = plt.subplot(1, len(classifiers), i + 1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, alpha=0.5)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(names[i])\n",
    "\n",
    "    print('accuracy {}: {}'.format(names[i], (prediction == y_test).sum() * 1. / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score(alphas, estimators, X, y):\n",
    "    return accuracy_score(y, predict(alphas, estimators, X))\n",
    "\n",
    "def predict(alphas, estimators, X, y=None):\n",
    "    return np.sign(sum(\n",
    "        b * e.predict(X) for b, e in zip(alphas, estimators)\n",
    "    ))\n",
    "\n",
    "def construct_alpha_history(init_alphas):\n",
    "    \"\"\"\n",
    "    construct alpha history, alike recomputed alpha history, for AdaBoost algorithm\n",
    "    \"\"\"\n",
    "    alphas = np.zeros((len(init_alphas), len(init_alphas)))\n",
    "    for t, alpha in enumerate(init_alphas):\n",
    "        alphas[t:, t] = alpha\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the resulting alphas (last calculated in case of recomputed alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the scores of the models at each iteration (each iteration - each estimator added to the ensemble) \n",
    "## for AdaBoost and RecomputedAdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a conclusion about which method is better and in which case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Gradient Boosting and Feature Selection (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machines (GBM) are a historical and logical continuation of the first boosting algorithms. In a way, one can consider AdaBoost as another variant of GBMs. These methods are the extremely powerful tools, widely used in industry, research and various machine learning competitions. \n",
    "\n",
    "In this task we offer to focus on one varinat of GBM called [XGBoost](https://github.com/dmlc/xgboost/tree/master/python-package). The dataset that is going to be used is  [Telecom Churn Dataset] (https://www.kaggle.com/becksddf/churn-in-telecoms-dataset). You will need to construct an XGBoost classification model, train it, plot the ROC curve, measure the training time and compare it to Random Forest. Afterwards, compare the models' feature importances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**\n",
    "* train the XGBoost classifier on the provided dataset\n",
    "  * measure the training time\n",
    "  * measure the precision/recall on the test set\n",
    "  * plot ROC-curve\n",
    "\n",
    "* train Random Forest classifier and compare it to XGBoost (plot ROC-curve)\n",
    "\n",
    "* compare the feature importances of the trained XGBoost and Random Forest Classifiers. Why do you think they are different? Explain.\n",
    "\n",
    "* for each model, sort the features by their importance, and plot the dependence of the test score on the number of features selected by best importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fit the models to the iteratively reduced subset of features -in order to fo that, firstly, sort the feature importances in the decreasing order; then fit the models on the iteratively reduced feature subsets corresponding to the currently chosen threshold of the feature importance and plot the precision-recall.\n",
    "\n",
    "**HINT**: you can use ```sklearn.feature_selection.SelectFromModel``` and  its ```.transform``` methods in order to get the new $X$,$y$ subsets (according to the current threshold of the feature importance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you have installed the XGBoost package before starting the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT**: in order to measure the training time you can use [**timeit** cell magic](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = pd.read_csv('data/telecom_churn.csv')\n",
    "cols = ['account length','number vmail messages',\n",
    "       'total day minutes', 'total day calls', 'total day charge',\n",
    "       'total eve minutes', 'total eve calls', 'total eve charge',\n",
    "       'total night minutes', 'total night calls', 'total night charge',\n",
    "       'total intl minutes', 'total intl calls', 'total intl charge',\n",
    "       'customer service calls', 'churn']\n",
    "data = data[cols]\n",
    "data.iloc[:,-1] = pd.Categorical(data.iloc[:,-1])\n",
    "data.iloc[:,-1] = data.iloc[:,-1].cat.codes\n",
    "X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=0x0BADBEEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train the XGBoost classifier on the provided dataset\n",
    "* measure the training time\n",
    "* measure the precision/recall on the test set\n",
    "* plot ROC-curve\n",
    "* train Random Forest classifier and compare it to XGBoost (plot ROC-curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* extract and compare the feature importances calculated by the previously trained XGBoost and Random Forest Classifiers. Are they different or the same, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the models to the iteratively reduced subset of features -in order to fo that, firstly, sort the feature importances in the decreasing order; then fit the models on the iteratively reduced feature subsets corresponding to the currently chosen threshold of the feature importance and plot the precision-recall. **HINT**: you can use ```sklearn.feature_selection.SelectFromModel``` and  its ```.transform``` methods in order to get the new $X$,$y$ subsets (according to the current threshold of the feature importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to compare the performance of the common Ridge Regression with its the parameters chosen using cross-validation technique and Bayesian Ridge Regression. \n",
    "\n",
    "In particular, using [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html), find the optimal value for the reguralization coefficient in the ridge regression optimization problem formulation. Then train [Bayesian Regression](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression) and compare the absolute error and learnt coefficients distribution for both variants and make some conclusions on both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task:** \n",
    "* Use crossvalidation technique to choose the optimal value of reguralization coefficent for Ridge Regression model. (**Note:** \"optimal\" value here, means the one that lets the model with this particular parameter value achieve better performance compared to the other variants). \n",
    "* Fit Bayesian Ridge Regression model \n",
    "* Plot the absolute error historgramms for both models using [seaborn.distplot](https://seaborn.pydata.org/generated/seaborn.distplot.html).\n",
    "* Plot historgrams of coefficients of regression derived from both models (in cross-validation case the coefficients of the best model)\n",
    "* Compare the two pairs of histrograms. Are they the same or different? Why? What are the prons and cons of both methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as dt\n",
    "from sklearn.model_selection import cross_validate as cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = dt.load_boston(return_X_y=True)\n",
    "X_train = X[:400]\n",
    "y_train = y[:400]\n",
    "X_test = X[400:]\n",
    "y_test = y[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Stacking (2 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is another approach to combine several algorithms to get better results.Basically stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The main concept is to learn base models on some set of features then train meta model, which uses the predictions of base models as features. This technique is wiledly used in multiple real case scenarios to get better results. One of the main problem of stacking is, of course, overfitting. To avoid it, the training set is divided into $ n $ folds, $ (n-1) $ of which are used for training the base models, and $ n $ -th for the overall prediction (calculating the meta-factor). In order to train the meta-model, it should receive predictions from the base models for those objects of the training sample on which the meta-model will be trained. \n",
    "\n",
    "There are various of stacking approaches$.^{[1]}$ For example, in order, to obtain meta-factors for test data, the base classifiers can be trained on the entire training set, since the problem of overfitting does not arise here. In other words, if we want to calculate the factors for the test set, we can safely use the training set to train the base classifiers. If we want to calculate factors for the training set, then it is necessary to ensure that the classifier does not predict for those objects on which it has been trained$.^{[2]}$ You can read more details about stacking [blockpost](https://blog.statsbot.co/ensemble-learning-d1dcd548e936), [kaggle ensemble guide](https://mlwave.com/kaggle-ensembling-guide/).\n",
    "\n",
    "P.s. Stacking and Blending are two similar approaches to combining classifiers (ensembling). The difference is that Stacking uses out-of-fold predictions for the train set, and Blending uses a validation set to train the next layer [source](quora.com/What-are-examples-of-blending-and-stacking-in-Machine-Learning). Because they are so similar you can use any of them. \n",
    "\n",
    "[1] http://www.machinelearning.ru/wiki/images/5/56/Guschin2015Stacking.pdf  \n",
    "[2] MLgroup SHAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task:**\n",
    "\n",
    "* Complete the ```meta_classfier``` function\n",
    "* Choose 6 different base models(base models can be any models that you know, and can differ between each other by different hyperparameters, models of ml,features e.t.c) and train them.\n",
    "* Report individual scores on test set for each of the models. As a score use accuracy.\n",
    "* Train metaclassifier on original datasets features, report score on test.\n",
    "* Train meta_classifier on those base models. Report the test score (accuracy) in this case.\n",
    "* Does stacking helped to gain better score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of theory, let's get back to Practice. Download dataset fetch_covertype from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype) and  split it train-test - 60/40.\n",
    "More detail about this dataset you can find [here](https://archive.ics.uci.edu/ml/datasets/Covertype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "fc = sklearn.datasets.fetch_covtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write meta classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_classifier(base_clfs, final_classifier, X_train, X_test, y_train, cv):\n",
    "    \"\"\"\n",
    "    Meta classifier prediction using stacking. \n",
    "    Input:\n",
    "    :param base_clfs: list,  base classifiers which will be stacked together.\n",
    "    :param final_classifier: estimator, a classifier which will be used to combine the base estimators. \n",
    "    :param X_train: numpy array or pandas table, train set.\n",
    "    :param X_test: numpy array or pandas table, target for train set.\n",
    "    :param X_train: numpy array or pandas table, test set.\n",
    "    \n",
    "    Output:\n",
    "    :param y_pred: numpy array or pandas table, prediction of meta classifier using stacking on test set.\n",
    "    :param final_classifier(optional): estimator, trained final_calssifier.\n",
    "    \n",
    "    \n",
    "    More details https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
    "    \n",
    "    \"\"\"\n",
    "    ###YOUR CODE###\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose 6 different base models(base models can be any models that you know,and can differ with each other by different hyperparameters,  models of ml,features e.t.c) and train them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report individual scores on test set. As a score use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train metaclassifier on original datasets features. And report score on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train meta_classifier on those base models ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report score(accuracy) on test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does stacking helped to gain better score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Gaussian Processes (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you will need to fit the gaussian process with different kernels on the ```elecricity_conpumption_FR.csv``` time series data. We have already loaded and prepared the train/test datasets for you below.\n",
    "\n",
    "Use [**GPy**](https://pypi.python.org/pypi/GPy) library for training and prediction. Fit a GP and run the predict on the test. Useful kernels to combine: `GPy.kern.RBF, GPy.kern.Poly, GPy.kern.StdPeriodic, GPy.kern.White, GPy.kern.Linear`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**:\n",
    "* Normalize the target value by fitting a transformation on the train (use ```sklearn.preprocessing.StandardScaler```)\n",
    "* Plot the resulting target against the time index.\n",
    "* Plot mean and confidence interval of the prediction. \n",
    "* Inspect them on normality by scatter plot: plot predicted points/time series against true values. \n",
    "* Estimate the prediction error with `r2_score`. R2-score accepted > 0.7 on test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/elecricity_conpumption_FR.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data.values)\n",
    "X = np.array(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.3)\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalize the target value by fitting a transformation on the train (use ```sklearn.preprocessing.StandardScaler```)\n",
    "* Plot the resulting target against the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "from GPy.kern import RBF, Poly, StdPeriodic, White, Linear,PeriodicExponential\n",
    "from sklearn.metrics import r2_score\n",
    "import GPy.kern as kern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot mean and confidence interval of the prediction. \n",
    "* Inspect them on normality by scatter plot: plot predicted points/time series against true values. \n",
    "* Estimate the prediction error with `r2_score`. R2-score accepted > 0.7 on test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
