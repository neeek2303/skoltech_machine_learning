{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(w|D_m) = \\frac {p(D_m|w)p(w)}{p(D_m)} \\\\\n",
    "\\text{MAP} : w*= \\arg\\max_{w}p(w|D_m)  = \\arg\\max_{w} \\log{p(w|D_m)} =  \\arg\\max_{w} { \\left[\\log{p(D_m|w)}+ \\log{p(w)}\\right]} \\\\\n",
    "p(D_m|w) = \\prod_{n=1}^{m} N(x_i^2,\\sigma^2) \\\\\n",
    "w* = \\arg\\max_{w} { \\left[\\sum_{n=1}^{m} \\log{N\\left(w^T x_i,  \\sigma^2\\right)} + \\log{N(0,\\alpha I)}\\right]} = \\\\ \n",
    "= \\arg\\max_{w} { \\left[ - \\frac{1}{2\\sigma^2}\\sum_{n=1}^{m} (w^Tx_i - y_i) - \\frac{m}{2}\\log(\\sigma^2) - \\frac{m}{2}(2\\pi) - \\frac{\\alpha}{2}ww^T + \\frac{(M+1)}{2}\\log{\\frac{\\alpha}{2\\pi}} \\right]} = \\\\\n",
    "= \\arg\\min_{w} { \\left[ \\frac{1}{2\\sigma^2}\\sum_{n=1}^{m} (w^Tx_i - y_i) + \\frac{\\alpha}{2}ww^T  \\right]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(f_*|y) = N(f_*|\\mu_*,\\sigma_*^2), \\quad \n",
    "\\sigma_n^2(x^*) = K_{**} - k_{n*}^T\\left[K + \\sigma^2I_m\\right]^{-1}k_{n*} \n",
    "\\\\ K_{**} = k(x^*,x^*), \\quad   k_{n*} = \\{k(x^*, x_i)\\}_{i=1}^n \\\\\n",
    "\\text{We have to prove that } k_{n*}^T\\left[K + \\sigma^2I_m\\right]^{-1}k_{n*} \\geq k_{n-1*}^T\\left[K + \\sigma^2I_m\\right]^{-1}k_{n-1*} \\\\\n",
    "\\begin{align}\n",
    "K_n + \\sigma^2 I_n = \n",
    "\\begin{pmatrix}\n",
    "K_{n-1} + \\sigma^2 I_{n-1} \\quad k_{n-1}(x')\\\\k_{n-1}(x')^\\top \\quad k(x',x') + \\sigma^2\n",
    "\\end{pmatrix}\n",
    "\\end{align}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taked from http://www.gaussianprocess.org/gpml/chapters/RW.pdf (C. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006) A.3 p. 201 (1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {A=\\left( \\begin{array}{cc}{P} \\quad {Q} \\\\ {R} \\quad {S}\\end{array}\\right), \\quad A^{-1}=\\left( \\begin{array}{cc}{\\tilde{P}} \\quad {\\tilde{Q}} \\\\ {\\tilde{R}} \\quad {\\tilde{S}}\\end{array}\\right)} \\\\\n",
    "\\text{where}\\\\\n",
    "\\tilde{P}=P^{-1}+P^{-1} Q M R P^{-1} \\\\ \n",
    "\\tilde{Q}=-P^{-1} Q M \\\\\n",
    "\\tilde{R}=-M R P^{-1} \\\\ \n",
    "\\tilde{S}=M\\\\\n",
    "M=\\left(S-R P^{-1} Q\\right)^{-1}\n",
    "$$\n",
    "\n",
    "$$ \\text{Used it we can get: }\\\\ \n",
    "\\begin{align}\n",
    "(K_n + \\sigma^2 I_n)^{-1} = \\begin{pmatrix}\\kappa + \\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa \\quad -\\kappa k_{n-1}(x')M \\\\ -Mk_{n-1}(x')^\\top \\kappa \\quad M \\end{pmatrix},\n",
    "\\end{align} \\\\\n",
    "\\text{where: } \\\\ \n",
    "M  = (k(x', x') + \\sigma^2 - k_{n-1}(x')^\\top(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x'))^{-1}, \\\\\n",
    "\\kappa  = (K_{n-1} + \\sigma^2I_{n-1})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{So, finally we get explicit formula for } k_n(x^*)^\\top(K_n + \\sigma^2I_n)^{-1}k_n(x^*) \\\\ \n",
    "k_{n}(x^*)^\\top(K_n + \\sigma^2I_n)^{-1}k_{n}(x^*) = \\quad \\begin{pmatrix}k_{n-1}(x^*)\\\\k'(x^*)\\end{pmatrix}^\\top\\begin{pmatrix}\\kappa + \\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa \\quad -\\kappa k_{n-1}(x')M \\\\ -Mk_{n-1}(x')^\\top\\kappa  \\quad M \\end{pmatrix}\\begin{pmatrix}k_{n-1}(x^*)\\\\k'(x^*)\\end{pmatrix}\\\\\n",
    "\\quad = \\begin{pmatrix} k_{n-1}^\\top(x^*)\\kappa + k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa - k'(x^*)Mk_{n-1}(x')^\\top \\kappa \\\\ -k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')M + k'(x^*)M \\end{pmatrix} ^\\top\n",
    "\\begin{pmatrix}k_{n-1}(x^*)\\\\k'(x^*)\\end{pmatrix} = k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x^*) + k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*)\\\\\n",
    "\\quad - k'(x^*)Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*) - k_{n-1}(x^*)^\\top\\kappa k_{n-1}(x')Mk'(x^*) + k'(x^*)Mk'(x^*)\n",
    "$$\n",
    "\n",
    "$$M  = \\left[k(x', x') + \\sigma^2 - k_{n-1}(x')^\\top(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x')\\right]^{-1} = [\\sigma_{n-1}^2(x') + \\sigma^2]^{-1} > 0 \n",
    "$$ \n",
    "\n",
    "$$\n",
    "k_{n}(x^*)^\\top(K_n + \\sigma^2I_n)^{-1}k_{n}(x^*) -   k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x^*) +k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*)\\\\\n",
    "\\quad - k'(x^*)Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*) - k_{n-1}(x^*)^\\top\\kappa k_{n-1}(x')Mk'(x^*) + k'(x^*)Mk'(x^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Then }\\\\\n",
    "k_{n}(x^*)^\\top(K_n + \\sigma^2I_n)^{-1}k_{n}(x^*) -   k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x^*) = \\\\\n",
    " = k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*)\\\\\n",
    "\\quad - k'(x^*)Mk_{n-1}(x')^\\top\\kappa k_{n-1}(x^*) - k_{n-1}(x^*)^\\top\\kappa k_{n-1}(x')Mk'(x^*) + k'(x^*)Mk'(x^*)  = \\\\\n",
    "= M *\\left[  k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x')k_{n-1}(x')^\\top\\kappa k_{n-1}(x^*)\\\\\n",
    "\\quad - k'(x^*)k_{n-1}(x')^\\top\\kappa k_{n-1}(x^*) - k_{n-1}(x^*)^\\top\\kappa k_{n-1}(x')k'(x^*) + k'(x^*)k'(x^*) \\right] = \\\\\n",
    " =  M *\\left(k_{n-1}^\\top(x^*)\\kappa k_{n-1}(x') - k'(x^*)\\right)^2 \\geq 0\n",
    "$$\n",
    "\n",
    "$$\\text{Thus } \\\\\n",
    "k_{n}(x^*)^\\top(K_n + \\sigma^2I_n)^{-1}k_{n}(x^*) \\geq   k_{n-1}^\\top(x^*)(K_{n-1} + \\sigma^2I_{n-1})^{-1} k_{n-1}(x^*) \\\\ \n",
    "\\text{and} \\\\\n",
    "\\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you have gaussian distribution on $R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$.Get an expression for the correlation between the implementation of a Gaussian process  $y(x) âˆ¼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$corr(y(x), \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}) =  \\frac{cov(y(x), \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x})}{\\sigma(y(x))\\sigma(\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x})} =  \\\\ [cov\\left(y(x), \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}\\right) = \\frac{\\partial k(x, \\tilde x)}{\\partial \\tilde x}; \\;  cov\\left( \\frac{\\partial y(x)}{\\partial x}, \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}\\right) = \\frac{\\partial^2 k(x, \\tilde x)}{\\partial x\\partial \\tilde x}, \\text{C. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006, (9.1) p.191}] \\\\ =   \\frac{ \\frac{\\partial }{\\partial \\tilde x}k(x, \\tilde x)}{k(x, x)\\frac{\\partial^2 }{\\partial \\tilde x^2}  k(\\tilde x, \\tilde x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{Distance= } ||x-x'||^2; K(x,x')= (\\phi(x),\\phi(x')) \\\\\n",
    "||\\phi(x)-\\phi(x')||^2 = ||\\phi(x)||^2 + ||\\phi(x')||^2 - 2(\\phi(x),\\phi(x')) = K(x,x) + K(x',x') -2K(x,x') \\\\ \n",
    "d(x,x') = \\sqrt{K(x,x) + K(x',x') -2K(x,x')}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the target vector $y$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**:\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$|$L$|$h_0$|$f_1$|$$y-f_1$$|$L$|$h_1$|$f_2$|$$y - f_2$$|$L$|$h_2$|$F_3$|\n",
    "|------|------|-----|-----------|---|-----|-----|---------|---|-----|-----|-----------|---|-----|-----|\n",
    "|  10  |  1   |  12.4  |-11.4| 53.44 |  -7.4  |  5  |-4| 16.93 |  -1.41  |  3.58  |-2.58| 8.91 |  -2.58  |  1  | \n",
    "|  32  |  9   |  12.4  |-3.4| 20.95 |  -7.4  |  5  |4| 12.93 |  -1.41  |  3.58  |  5.42  | 7.24 |  0.65  |  4.22  |\n",
    "|  46  |  13  |  12.4  |0.6| 16.93 |  4.93  |  17.33  | -4.33 | 16.93 |  -1.41  |  15.92  | -2.92| 7.57 |  0.65  |  16.56  |\n",
    "|  54  |  16  |  12.4  |3.6| 19.83 |  4.93  |  17.33  |-1.33| 13.80 |  -1.41  |  15.92  |0.08| 8.90 |  0.65  |  16.56  |\n",
    "|  63  |  23  |  12.4  |10.6| 23.35 |  4.93  |  17.33  |5.67| 8.91 |  5.67  |  23  |0| 8.91 |  0.65  |  23.65  |\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 32, 46, 54, 63])\n",
    "y = np.array([1, 9, 13, 16, 23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([x, y], dtype=float).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_split_loss(index, errors):\n",
    "    er_left = 0 if index==0 else ((errors[:index] -errors[:index].mean())**2).sum()\n",
    "    er_right = ((errors[index:] -errors[index:].mean())**2).sum()  \n",
    "    loss = (er_left+er_right)/errors.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_val_update(index, errors):\n",
    "    new_errors = np.zeros(residuals.shape[0])\n",
    "    new_errors[:index] = errors[:index].mean()\n",
    "    new_errors[index:] = errors[index:].mean()\n",
    "    return new_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data.shape[0] \n",
    "ITERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_indexes = np.zeros(ITERS, dtype=int)\n",
    "\n",
    "function_values = np.zeros((N, ITERS+1), dtype=float)\n",
    "function_values[:,0] = data[:,1].mean() # f_0\n",
    "\n",
    "errors = np.zeros([N, ITERS+1])\n",
    "errors[:,0] = data[:,1] - data[:,1].mean() # Initial error y-f_0\n",
    "\n",
    "learner_values = np.zeros((N, ITERS), dtype=float) # h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53.44       20.95       16.93333333 19.83333333 25.35      ]\n",
      "[16.93333333 12.93333333 16.93333333 13.8037037   8.90555556]\n",
      "[8.90555556 7.23715278 7.56759259 8.90439815 8.90555556]\n"
     ]
    }
   ],
   "source": [
    "for learner_index in range(ITERS):\n",
    "    loss_splits = np.zeros(N)    \n",
    "    \n",
    "    # Calculate L_i\n",
    "    for split_index in range(N):\n",
    "        loss_splits[split_index] = L_split_loss(split_index, errors[:,learner_index])    \n",
    "    print(loss_splits)\n",
    "    \n",
    "    # calculate h_i values\n",
    "    split_indexes[learner_index] = np.argmin(loss_splits)\n",
    "    learner_values[:,learner_index] = lr_val_update(split_indexes[learner_index], errors[:,learner_index])\n",
    "    \n",
    "    # Calculate f_i\n",
    "    function_values[:,learner_index+1] = function_values[:, learner_index] + learner_values[:,learner_index]\n",
    "    \n",
    "    # Calculate y - f_i\n",
    "    errors[:,learner_index+1] = errors[:,learner_index]- learner_values[:,learner_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.4       , -1.41666667, -2.58333333],\n",
       "       [-7.4       , -1.41666667,  0.64583333],\n",
       "       [ 4.93333333, -1.41666667,  0.64583333],\n",
       "       [ 4.93333333, -1.41666667,  0.64583333],\n",
       "       [ 4.93333333,  5.66666667,  0.64583333]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.4       ,  5.        ,  3.58333333,  1.        ],\n",
       "       [12.4       ,  5.        ,  3.58333333,  4.22916667],\n",
       "       [12.4       , 17.33333333, 15.91666667, 16.5625    ],\n",
       "       [12.4       , 17.33333333, 15.91666667, 16.5625    ],\n",
       "       [12.4       , 17.33333333, 23.        , 23.64583333]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11.4       ,  -4.        ,  -2.58333333,   0.        ],\n",
       "       [ -3.4       ,   4.        ,   5.41666667,   4.77083333],\n",
       "       [  0.6       ,  -4.33333333,  -2.91666667,  -3.5625    ],\n",
       "       [  3.6       ,  -1.33333333,   0.08333333,  -0.5625    ],\n",
       "       [ 10.6       ,   5.66666667,   0.        ,  -0.64583333]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return F_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([10,32,46,54,63])\n",
    "y = np.array([1, 9, 13, 16, 23])\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "F3 = function_values[:,3]\n",
    "splits = x[split_indexes]\n",
    "x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "boosted_F_plot = plot_tree(x, F3, stumps = list(np.sort(splits)))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "ax.plot(x_range,boosted_F_plot,'--', linewidth=2, label = 'composite function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data[:,0], data[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text {Weights of classifers in AdaBoost: } \\alpha_t = \\frac{1}{2}\\log{\\frac{1-N_t}{N_t}} \\\\\n",
    "N_t = \\frac{1}{2}: \\alpha_t = \\frac{1}{2}\\log{1} = 0, \\text{It is mean that $f_t(x)$ dont play any role in classification } \\\\ \n",
    "N_t \\geq \\frac{1}{2}: \\alpha_t = \\frac{1}{2}\\log{b} [b\\leq1] \\leq 0, \\text{It is mean that $f_t(x)$ play any role in classification, but it is good in misclassifications} \\\\ \n",
    "N_t = 0: \\alpha_t = \\frac{1}{2}\\log{\\frac{1}{0}} \\to \\inf, \\text{It is mean that $f_t(x)$ is 'perfect' classifier, but in practice we can't get $N_t=0$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
