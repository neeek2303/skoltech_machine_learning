{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
