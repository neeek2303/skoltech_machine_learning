{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:\n",
    "Let's consider:\n",
    "$$\n",
    "||XA\\tilde{w} - y||_2^2 + \\lambda||\\tilde{w}||_{1}\\\\\n",
    "\\text{and}\\\\\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty}\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ Want the first task is equivalent to the second, then:}\\\\\n",
    "\\\\\n",
    "||Aw||_{1} = ||w||_{\\infty}\\\\ \n",
    "$$\n",
    "\n",
    "So we need to find coordinate transformation A, such that L1 goes to norm infinity norm , in this case Task №3 of this homework help as. For geometric we can see that we need flip space 45 degrees and increas it in $\\sqrt{2}$ times\n",
    "\n",
    "$$\n",
    "A = \\sqrt{2}*\\begin{pmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta)\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 & -1\\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:\n",
    "We need to solve this problem:\n",
    "\n",
    "$ L(w) = \\sum\\limits_{i=1}^m \\log\\left(\\int_{-\\infty}^{(x^Tw + b)y_{i}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt\\right) \\rightarrow \\max_{\\boldsymbol{w, b}} $\n",
    "\n",
    "If the training set is linearly separable, it means we can divide objects of two classes by hyperplane $\\tilde{w^\\top} x + \\tilde{b} = 0$. It means that for any $\\gamma \\in \\mathbb{R}$  $\\lambda\\tilde{w^\\top} x + \\lambda\\tilde{b} = 0$ will give such separating hyperplane, so, there will be an infinite number of solutions and $\\|\\lambda\\tilde{w}\\|, |\\lambda\\tilde{b}| \\to \\infty , \\lambda \\to \\infty$. And it also mean that $y_i(x^Tw + b)>0$ for all $i$.\n",
    "\n",
    "$\\sum\\limits_{i=1}^m \\log \\int_{-\\infty}^{y_i(x^Tw + b)}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt \\to_{\\lambda\\rightarrow +\\infty} \\left(\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt =1 \\right) \\to 0 $ \n",
    "\n",
    "This means that one can make $L(w)$ as close to $0+$ as required. On the other hand, $\\forall{w}$ we observe $L({w})>0$. Thus, the infimum of $0$ and it is not achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:\n",
    "For Bayesian Naive Classifier and binary-valued features we can write a conditional probability:$$ p(C_k|x) \\sim p(C_k)p(x|C_k) = p(C_k)\\prod_{j=1}^{d}p_{kj}^{x_j}(1-p_{kj})^{(1-x_j)} $$  $p_{kj}$ is the probability of class $C_{k}$ generating the term $x_{j}$.\n",
    "\n",
    "We can write decision rule as follows: $$f(x) = \\arg\\max_{C_k} {p(C_k|x)} = \\big[\\textbf{apply} \\log\\big] =  \\arg\\max_{C_k}\\log \\left(p(C_k)\\prod_{j=1}^{d}p_{kj}^{x_j}(1-p_{kj})^{(1-x_j)}\\right) = \\arg\\max_{C_k} \\left(log(p(C_k)) + \\sum \\limits_{j=1}^{d}(x_i \\log p_{kj} + (1-x_j)\\log(1 - p_{kj})) \\right) = \\\\ = \\arg\\max_{C_k} \\left( \\log p(C_k) + \\sum \\limits_{j=1}^{d}(x_j \\log \\frac{p_{kj}}{\\log(1 - p_{kj})} + \\log(1-p_{kj})) \\right), $$ from last equation we see, that probability in log-scale is linear respect to $x_j$, so we have linear decision rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV= \\frac{1}{C_{n}^{k}} \\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose we know $m_i$ for every $y_i \\in X'$,  then we have\n",
    "\n",
    "$$ L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq y_i^{m_i}]\\bigg)\\bigg] $$\n",
    "Now we can calculate how many subsets of $X^n$ have $m-1$ closet neighbors. So it is $m-k-1$ of elements, who is not in $X'$ from n-m-1(we know that $m-th$ closest neighbours not in $X'$), The number of such partitions is exactly equal to $C_{n-m-1}^{n-k-1}$ because we should take $n-k-1$ elements for control subsample and $n-m-1$ in another subsample( $m$ closest neighbours and $y$)\n",
    "\n",
    "$$\n",
    "L_{k}OCV=\\sum_{i=1}^{n}\\frac{1}{C_n^kk}\\sum_{m=1}^{k}C_{n-m-1}^{n-k-1}[y_i\\neq y_i^m]\n",
    "$$\n",
    "it is will be the same that\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{k}\\frac{C_{n-m-1}^{n-k-1}}{C_n^kK}\\sum_{i=1}^{n}[y_i\\neq y_i^m]=\\sum_{m=1}^{k}\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq y_i^m]\\frac{C_{n-k-1}^{n-1-m}}{C_{n-1}^{k-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "\n",
    "Bootstrap selects a point randomly and replaces its copy back to the dataset keeping it unchanged, and it makes this selection $N$ times providing as a result $N$ points, some of which can have their copies among the selected points. Therefore, the propability of any point to be selected after one choice is $1/N$, not to be selected $1-1/N$. So, after $N$ choices we have\n",
    "$$P_{\\hat{X}^n} = \\left(P_{\\hat{x_i}}\\right)^n = \\left(1 - \\frac{1}{n}\\right)^n$$.\n",
    "The answer for our problem is a probability of a subset with \"our\" $x_i$, that equals:$$P_{final} = 1 - P_{\\hat{X}^n} = 1 - \\left(1 - \\frac{1}{n}\\right)^n$$\n",
    "Reviewing assimptotical case, we need to find the limit:$$\\lim\\limits_{n \\rightarrow \\infty} \\left(1 - \\left(1 - \\frac{1}{n}\\right)^n\\right) = 1 - \\lim\\limits_{n \\rightarrow \\infty}\\left(1 - \\frac{1}{n}\\right)^n  = \\text{assuming} \\lim\\limits_{n \\rightarrow \\infty}\\left(1 + \\frac{\\alpha}{n}\\right)^n = e^{\\alpha} =   1 - e^{-1}$$\n",
    "\n",
    "$$ \\lim_{N\\to\\infty} \\left(1-\\frac{1}{N}\\right)^N = \\frac{1}{e}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:\n",
    "$ 1. \\hat y = \\min_{c}  \\sum_{(x_i,y_i \\in S)} L(c) $\n",
    "\n",
    "$2. L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}  \\to \\hat R = \\sum_{i=1}^{n} \\frac{(y_i-\\hat{y})^{2}}{y_i^2}$\n",
    "\n",
    "$ \\frac{\\partial \\hat R}{\\partial \\hat y} = 2 \\sum_{i=1}^{n} \\frac{(\\hat{y} - y_i)}{y_i^2} = 0 \\to \\hat y = \\frac{\\sum_{i=1}^{n} \\frac{1}{y_i}}{\\sum_{i=1}^{n} \\frac{1}{y_i^2}}  $ \n",
    "\n",
    "$ \\frac{\\partial^{2} \\hat R}{\\partial \\hat y^2} = 2 \\sum_{i=1}^{n} \\frac{1}{y_i^2} > 0  \\to \\text{It is a minimum} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:\n",
    "\n",
    "$$ROCAUC =  \\sum_{j}\\text{TPR}_j\\Delta \\text{FPR}_j= \\\\ \\left[w_0\\in\\{f(x_i)\\}_{i=1}^n, \\Delta \\text{FPR}_j = \\frac{1}{n_0}1_{y_j=0}1_{f(x_{j+1})\\geq f(x_j)}=\\frac{1}{n_0}1_{y_j=0} = \\frac{1}{n-n_1}1_{y_j=0},\\text{TPR}_j=\\sum_{i=1}^{j}\\frac{1}{n_1}1_{y_i=1}\\right] \\\\ = \\frac{1}{n_1(n-n_1)}\\sum_{j}\\sum_{i=1}^{j}1_{y_i=1}1_{y_j=0} = \\frac{1}{n_1(n-n_1)} \\sum_{i<j}[y_{i}>y_{j}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:\n",
    "### Case 1\n",
    "\n",
    "$$K(x,y)=\\langle x, y \\rangle ^ d = \\left(\\sum_{i=1}^N x_iy_i \\right)^d = \\sum_{d=1}^d \\left(\\sum_{i_d=1}^N x_iy_i \\right) = \\sum_{i_d=1}^{N} x_{i_d}y_{i_d}=\\sum_{i_1=1}^{N}...\\sum_{i_d=1}^{N} \\underbrace {(x_{i_1}...x_{i_d})}_{\\psi_{\\overrightarrow i}(x)}  \\underbrace {(y_{i_1}...y_{i_d})}_{\\psi_{\\overrightarrow i}(y)}= \\langle \\psi(x), \\psi(y) \\rangle  $$\n",
    "\n",
    "\n",
    "\n",
    "Let's find  $\\psi(x)$ .\n",
    "\n",
    "We want to get  K in this form $K(x,y)=\\sum_{\\overrightarrow r =(r_1,..,r_N)}  \\alpha_{r_1,...,r_N}x_1^{r_1}...x_N^{r_N}y_1^{r_1}...y_N^{r_N}=\\sum_{\\overrightarrow{r}}{\\psi_{\\overrightarrow r}(x)}{\\psi_{\\overrightarrow r}(y)}$\n",
    "We already know: $k(x,y)=(\\sum_{i_1=1}^{N}...\\sum_{i_d=1}^{N} \\underbrace {(x_{i_1}...x_{i_d})}_{\\psi_{\\overrightarrow i}(x)}  \\underbrace {(y_{i_1}...y_{i_d})}_{\\psi_{\\overrightarrow i}(y)})$ \n",
    "\n",
    "$x=(x_1,x_2,...,x_N)$ \n",
    "One factor in $K(x,y)$ сan be written as $x_1^{r_1}...x_N^{r_N}$ where $r_1+r_2+..+x_N=d, r_i \\in [0,d],  {\\overrightarrow r =(r_1,..,r_N)} $ \n",
    "\n",
    "The number of the possible ${\\overrightarrow r}$ vectors: $\\begin{pmatrix}\n",
    "  N+d-1 \\\\\n",
    "  d\n",
    "\\end{pmatrix}$ because $ r_1+r_2+..+x_N=d, r_i \\in [0,d] $\n",
    "\n",
    "that's why number of factors $dim(K)=\\begin{pmatrix}\n",
    "  N+d-1 \\\\\n",
    "  d\n",
    "\\end{pmatrix}$ \n",
    "\n",
    "\n",
    "The ${\\overrightarrow r =(r_1,..,r_N)}$  The all possible combimation  of power vector $r$ \n",
    "$\\alpha_{r_1,...,r_N}= \\frac {d!}{r_1!...r_N!} $ times \n",
    " \n",
    "\n",
    " ${\\psi_{\\overrightarrow r}(x)}= \\sqrt{\\frac {d!}{r_1!...r_N!}}x_1^{r_1}...x_N^{r_N} \\\\\n",
    " {\\psi_{\\overrightarrow r}(y)}= \\sqrt{\\frac {d!}{r_1!...r_N!}}y_1^{r_1}...y_N^{r_N}$\n",
    " \n",
    "\n",
    " \n",
    " That's why $K(x,y)=\\sum_{\\overrightarrow r =(r_1,..,r_N)}  \\alpha_{r_1,...,r_N}x_1^{r_1}...x_N^{r_N}y_1^{r_1}...y_N^{r_N}=\\sum_{\\overrightarrow{r}}{\\psi_{ i}(x)}{\\psi_{i}(y)}$\n",
    " \n",
    " ### Case 2\n",
    " \n",
    " In the second task, let's denote $\n",
    "        x'=\\begin{bmatrix}\n",
    "            \\sqrt(c) &x_1&x_2&...&x_N\n",
    "           \\end{bmatrix} ,   \n",
    "          y'=\\begin{bmatrix}\n",
    "            \\sqrt(c) &y_1&y_2&...&y_N\n",
    "           \\end{bmatrix}\n",
    "     $\n",
    "Use this $x',y'$ we can solve the problem like the first case\n",
    "${\\psi_{\\overrightarrow r}(x')}= \\sqrt{\\frac {d!}{r_1!...r_{N+1}!}}c^{r_1/2}x_1^{r_2}...x_N^{r_{N+1}}$\n",
    "\n",
    "The all possible combimation  of power vector $r$  \n",
    "\n",
    "$\\alpha_{r_1,...,r_{N+1}}= \\frac {d!}{r_1!...r_{N+1!}} $ times\n",
    " \n",
    "Sourse : http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/kernel_methods.pdf p.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:\n",
    "\n",
    "\n",
    "$ k(x,y)=\\exp(−\\frac{(x−y)^2}{2\\sigma^2}). \\text{Without loss of generality, suppose that }\\sigma^2=1$\n",
    "\n",
    "$k(x,y)=h(x−y), \\text{where } h(t)=\\exp(−\\frac{t^2}{2})=E[\\exp^{itZ}] \\text{\n",
    "is the characteristic function of a random variable Z with N(0,1) distribution.}$\n",
    "\n",
    "For real numbers $x1,…,xn$ and $a1,…,an$, we have\n",
    "$$\\sum_{j,k=1}^na_ja_kh(x_j−x_k)=\\sum_{j,k=1}^na_ja_kE\\left[\\exp^{i(x_j−x_k)Z}\\right] =E\\left[\\sum_{j,k=1}^na_j\\exp^{ix_jZ}a_k\\exp^{−ix_kZ}\\right]= E\\left[\\left|\\sum_{j=1}^na_j\\exp^{ix_jZ}\\right|^2\\right]≥0,$$\n",
    "\n",
    "thus Matrix K is SPSD and K is PDS kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
